{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(L):\n",
    "    \"\"\"\n",
    "    :type L: int(system size)\n",
    "    :rtype train,test: pd.dataframe(with phases labeled)\n",
    "    \"\"\"\n",
    "    train_filename='configuration'+str(L)+'.csv'\n",
    "    test_filename='test'+str(L)+'.csv'\n",
    "    columns=['temperature']+['Spin'+str(i) for i in xrange(1,L*L+1)]\n",
    "    train=pd.read_csv('Ising_data/'+'L'+str(L)+'/'+train_filename,names=columns)\n",
    "    test=pd.read_csv('Ising_data/'+'L'+str(L)+'/'+test_filename,names=columns)\n",
    "    \n",
    "    # decide critical temperature\n",
    "    filename='Cv'+str(L)+'.csv'\n",
    "    specific_heat=pd.read_csv('Ising_data/'+'L'+str(L)+'/'+filename,names=['temperature','Cv'])\n",
    "    Tc=specific_heat['temperature'][np.argmax(specific_heat['Cv'])]\n",
    "    \n",
    "    # add phase column\n",
    "    train['FM']=[int(T<=Tc) for T in train['temperature']]\n",
    "    test['FM']=[int(T<=Tc) for T in test['temperature']]\n",
    "    train['PM']=[int(T>Tc) for T in train['temperature']]\n",
    "    test['PM']=[int(T>Tc) for T in test['temperature']]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test=read_data(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_process(train, test):\n",
    "    \"\"\"\n",
    "    :type train,test: pd.dataframe(with phases labeled)\n",
    "    :rtype trX, trY, teX, teY\n",
    "    \"\"\"\n",
    "    # shuffle\n",
    "    train=train.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    trX, trY = train.drop(['FM','PM','temperature'],axis=1), np.array(train[['FM','PM']])\n",
    "    teX, teY = test.drop(['FM','PM','temperature'],axis=1), np.array(test[['FM','PM']])\n",
    "    \n",
    "    return trX, trY, teX, teY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX, trY, teX, teY=data_process(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tf_logistic(object):\n",
    "    log_likelihood_all=[]\n",
    "    sess=tf.Session()\n",
    "    def __init__(self,batch_size=100, max_iter=10, eta=0.02):\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter=max_iter\n",
    "        self.eta=eta\n",
    "        \n",
    "    def init_weights(self, shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "    \n",
    "    def init_bias(self, shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.05))\n",
    "    \n",
    "    def model(self, X, w, b):\n",
    "        return tf.matmul(X, w)+b\n",
    "    \n",
    "    def train(self,trX,trY,log=False):\n",
    "        self.X = tf.placeholder(\"float\", [None, trX.shape[1]]) # create symbolic variables\n",
    "        self.Y = tf.placeholder(\"float\", [None, trY.shape[1]])\n",
    "        \n",
    "        self.w = self.init_weights([trX.shape[1], trY.shape[1]])\n",
    "        self.b = self.init_bias([trY.shape[1]])\n",
    "        \n",
    "        self.py_x = self.model(self.X, self.w, self.b)\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.py_x, labels=self.Y))\n",
    "        train_op = tf.train.GradientDescentOptimizer(self.eta).minimize(cost)\n",
    "        \n",
    "        # at predict time, evaluate the argmax of the logistic regression\n",
    "        predict_op=tf.argmax(self.py_x,1)\n",
    "        \n",
    "        # you need to initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.log_likelihood_all=[]\n",
    "        \n",
    "        for i in xrange(self.max_iter):\n",
    "            for start, end in zip(range(0, len(trX), self.batch_size), \\\n",
    "                              range(self.batch_size, len(trX)+1, self.batch_size)):\n",
    "                self.sess.run(train_op, feed_dict={self.X: trX[start:end], self.Y: trY[start:end]})\n",
    "                self.log_likelihood_all.append(self.sess.run(cost,feed_dict={self.X: trX[start:end], \\\n",
    "                                                               self.Y: trY[start:end]}))\n",
    "            if log:    \n",
    "                print (i, np.mean(np.argmax(trY, axis=1) == \\\n",
    "                        self.sess.run(predict_op, feed_dict={self.X: trX, self.Y: trY}))),\\\n",
    "            \n",
    "    def predict(self, dataX, dataY, prob=False):\n",
    "        if prob:\n",
    "            return self.sess.run(tf.nn.sigmoid(self.py_x), feed_dict={self.X: dataX, self.Y: dataY})\n",
    "        else:\n",
    "            return self.sess.run(tf.argmax(self.py_x,1), feed_dict={self.X: dataX, self.Y: dataY})\n",
    "        \n",
    "    def score(self, y_pred, y_true):\n",
    "        return np.mean(y_pred==y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic=tf_logistic()\n",
    "logistic.train(trX,trY,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic.predict(trX,trY,prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=logistic.predict(trX,trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic.score(y_pred,np.argmax(trY, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood_log=logistic.log_likelihood_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def tf_logistic(trX, trY, teX, teY, batch_size=100, max_iter=20, eta=0.001):\n",
    "#     def init_weights(shape):\n",
    "#         return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "#     def init_bias(shape):\n",
    "#         return tf.Variable(tf.random_normal(shape,stddev=0.05))\n",
    "#     # The same function from linear regression\n",
    "#     def model(X, w, b):\n",
    "#         return tf.matmul(X, w)+b\n",
    "#     X = tf.placeholder(\"float\", [None, trX.shape[1]]) # create symbolic variables\n",
    "#     Y = tf.placeholder(\"float\", [None, trY.shape[1]])\n",
    "    \n",
    "#     w = init_weights([trX.shape[1], trY.shape[1]])\n",
    "#     b = init_bias([trY.shape[1]])\n",
    "    \n",
    "#     py_x = model(X, w, b)\n",
    "    \n",
    "#     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "#     train_op = tf.train.GradientDescentOptimizer(eta).minimize(cost) # construct optimizer\n",
    "    \n",
    "#     # at predict time, evaluate the argmax of the logistic regression\n",
    "#     predict_op=tf.argmax(py_x,1)\n",
    "    \n",
    "#     sess = tf.Session()\n",
    "#     # you need to initialize all variables\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     log_likelihood_all=[]\n",
    "    \n",
    "#     for i in xrange(max_iter):\n",
    "#         for start, end in zip(range(0, len(trX), batch_size), \\\n",
    "#                               range(batch_size, len(trX)+1, batch_size)):\n",
    "#             sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "#             log_likelihood_all.append(sess.run(cost,feed_dict={X: trX[start:end], \\\n",
    "#                                                                Y: trY[start:end]}))\n",
    "        \n",
    "#         print (i, np.mean(np.argmax(trY, axis=1) == \\\n",
    "#                         sess.run(predict_op, feed_dict={X: trX, Y: trY}))),\\\n",
    "#         (i, np.mean(np.argmax(teY, axis=1) ==\n",
    "#                         sess.run(predict_op, feed_dict={X: teX, Y: teY})))\n",
    "        \n",
    "#     return log_likelihood_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch_size=100\n",
    "# log_likelihood_log=tf_logistic(trX, trY, teX, teY, batch_size=batch_size, eta=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tf_fnn(object):\n",
    "    \n",
    "    log_likelihood_all=[]\n",
    "    sess=tf.Session()\n",
    "    \n",
    "    def __init__(self, lamb=0.05, hidden_units=100, batch_size=100, max_iter=30, eta=0.001):\n",
    "        self.lamb=lamb\n",
    "        self.hidden_units=hidden_units\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter=max_iter\n",
    "        self.eta=eta\n",
    "        \n",
    "    def init_weights(self, shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "    \n",
    "    def init_bias(self, shape):\n",
    "        return tf.Variable(tf.random_normal(shape,stddev=0.05))\n",
    "    \n",
    "    def layers(self,X, W, b):\n",
    "        return tf.nn.sigmoid(tf.matmul(X, W)+b)\n",
    "    \n",
    "    def model(self, X, w, b):\n",
    "        return tf.matmul(X, w)+b\n",
    "    \n",
    "    def train(self,trX,trY,log=False):\n",
    "        self.X = tf.placeholder(\"float\", [None, trX.shape[1]]) # create symbolic variables\n",
    "        self.Y = tf.placeholder(\"float\", [None, trY.shape[1]])\n",
    "        \n",
    "        self.w_1 = self.init_weights([trX.shape[1], self.hidden_units])\n",
    "        self.b_1 = self.init_bias([self.hidden_units])\n",
    "        self.w_2 = self.init_weights([self.hidden_units, trY.shape[1]])\n",
    "        self.b_2 = self.init_bias([trY.shape[1]])\n",
    "        \n",
    "        self.O1 = self.layers(self.X, self.w_1, self.b_1)\n",
    "        # self.O2 = self.layers(self.O1, self.w_2, self.b_2)\n",
    "        self.output = self.model(self.O1, self.w_2, self.b_2)\n",
    "        \n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.output, labels=self.Y)\\\n",
    "                                       +self.lamb*(tf.nn.l2_loss(self.w_1)+tf.nn.l2_loss(self.w_2)))\n",
    "        #cross_entropy = tf.reduce_sum(-self.Y*tf.log(self.O2)-(1.0-self.Y)*tf.log(1.0-self.O2)\\\n",
    "                                  #+self.lamb*(tf.nn.l2_loss(self.w_1)+tf.nn.l2_loss(self.w_2)))\n",
    "        \n",
    "        #train_op = tf.train.GradientDescentOptimizer(self.eta).minimize(cross_entropy)\n",
    "        train_op = tf.train.AdamOptimizer(self.eta).minimize(cross_entropy) # construct an optimizer\n",
    "        \n",
    "        #predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output,1), tf.argmax(self.Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        # you need to initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.log_likelihood_all=[]\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            for start, end in zip(range(0, len(trX), self.batch_size),\\\n",
    "                              range(self.batch_size, len(trX)+1, self.batch_size)):\n",
    "                self.sess.run(train_op, feed_dict={self.X: trX[start:end], self.Y: trY[start:end]})\n",
    "                self.log_likelihood_all.append(self.sess.run(cross_entropy, \\\n",
    "                                                             feed_dict={self.X: trX[start:end], self.Y: trY[start:end]}))\n",
    "                #self.log_likelihood_all.append(1./float(self.batch_size)*self.sess.run(cross_entropy,\\\n",
    "                                    #feed_dict={self.X: trX[start:end], self.Y: trY[start:end]}))\n",
    "            train_accuracy = self.sess.run(accuracy,feed_dict={self.X: trX, self.Y: trY})\n",
    "            \n",
    "            if log:\n",
    "                print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "                \n",
    "    def predict(self, dataX, dataY, prob=False):\n",
    "        if prob:\n",
    "            return self.sess.run(tf.nn.softmax(self.output), feed_dict={self.X: dataX, self.Y: dataY})\n",
    "        else:\n",
    "            return self.sess.run(tf.argmax(self.output,1), feed_dict={self.X: dataX, self.Y: dataY})\n",
    "        \n",
    "    def score(self, y_pred, y_true):\n",
    "        return np.mean(y_pred==y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def tf_fnn(trX, trY, teX, teY, lamb=0.05,hidden_units=100, batch_size=100, max_iter=20, eta=0.001):\n",
    "#     def init_weights(shape):\n",
    "#         return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "#     def init_bias(shape):\n",
    "#         return tf.Variable(tf.random_normal(shape,stddev=0.05))\n",
    "#     def layers(X, W, b):\n",
    "#         return tf.nn.sigmoid(tf.matmul(X, W)+b)\n",
    "    \n",
    "#     X = tf.placeholder(\"float\", [None, trX.shape[1]]) # create symbolic variables\n",
    "#     Y = tf.placeholder(\"float\", [None, trY.shape[1]])\n",
    "    \n",
    "#     w_1 = init_weights([trX.shape[1], hidden_units])\n",
    "#     b_1 = init_bias([hidden_units])\n",
    "#     w_2 = init_weights([hidden_units, trY.shape[1]])\n",
    "#     b_2 = init_bias([trY.shape[1]])\n",
    "#     O1 = layers(X, w_1, b_1)\n",
    "#     O2 = layers(O1, w_2, b_2)\n",
    "    \n",
    "#     cross_entropy = tf.reduce_sum(-Y*tf.log(O2)-(1.0-Y)*tf.log(1.0-O2)\\\n",
    "#                                   +lamb*(tf.nn.l2_loss(w_1)+tf.nn.l2_loss(w_2)))\n",
    "#     # train_op = tf.train.AdamOptimizer(eta).minimize(cross_entropy) # construct an optimizer\n",
    "#     train_op = tf.train.GradientDescentOptimizer(eta).minimize(cross_entropy)\n",
    "#     #predictions\n",
    "#     correct_prediction = tf.equal(tf.argmax(O2,1), tf.argmax(Y,1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "#     sess = tf.Session()\n",
    "#     # you need to initialize all variables\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     log_likelihood_all=[]\n",
    "#     for i in range(max_iter):\n",
    "#         for start, end in zip(range(0, len(trX), batch_size),\\\n",
    "#                               range(batch_size, len(trX)+1, batch_size)):\n",
    "#             sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "#             log_likelihood_all.append(1./float(batch_size)*sess.run(cross_entropy,\\\n",
    "#                                                feed_dict={X: trX[start:end], Y: trY[start:end]}))\n",
    "#         train_accuracy = sess.run(accuracy,feed_dict={X: trX, Y: trY})\n",
    "#         test_accuracy = sess.run(accuracy,feed_dict={X: teX, Y: teY})\n",
    "#         print \"step %d, training accuracy %g, test accuracy %g\"%(i, train_accuracy, test_accuracy)\n",
    "        \n",
    "#     return log_likelihood_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "hidden_units=3\n",
    "max_iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log_likelihood_fnn=tf_fnn(trX, trY, teX, teY, batch_size=batch_size, eta=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnn=tf_fnn(batch_size=batch_size,hidden_units=hidden_units,max_iter=max_iter,eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnn.train(trX,trY,log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_likelihood_fnn=fnn.log_likelihood_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def make_plot(log_likelihood_all, len_data, batch_size, smoothing_window=1, label=''):\n",
    "    plt.rcParams.update({'figure.figsize': (9,5)})\n",
    "    log_likelihood_all_ma = np.convolve(np.array(log_likelihood_all), \\\n",
    "                                        np.ones((smoothing_window,))/smoothing_window, mode='valid')\n",
    "    plt.plot(np.array(range(smoothing_window-1, len(log_likelihood_all)))*float(batch_size)/len_data,\n",
    "             log_likelihood_all_ma, linewidth=4.0, label=label)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('# of passes over data')\n",
    "    plt.ylabel('Average log likelihood per data point')\n",
    "    plt.legend(loc='lower right', prop={'size':14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_plot(log_likelihood_log[:len(log_likelihood_log)], trX.shape[0], batch_size, 50, label='logistic')\n",
    "make_plot(log_likelihood_fnn[:len(log_likelihood_log)], trX.shape[0], batch_size, 50, label='fnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood_fnn={}\n",
    "for hidden_units in [10,50,100,200,400]:\n",
    "    fnn=tf_fnn(hidden_units=hidden_units, eta=0.002)\n",
    "    fnn.train(trX,trY,log=True)\n",
    "    log_likelihood_fnn[hidden_units]=fnn.log_likelihood_all\n",
    "    #log_likelihood_fnn[hidden_units]= \\\n",
    "    #tf_fnn(trX, trY, teX, teY, hidden_units=hidden_units, batch_size=batch_size, eta=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('log_likelihood_fnn.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter='\\t')\n",
    "    for key,value in log_likelihood_fnn.items():\n",
    "        writer.writerow([key]+value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "my_dict={}\n",
    "with open('log_likelihood_fnn.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        key=int(row[0])\n",
    "        value=[float(e) for e in row[1:]]\n",
    "        my_dict[key]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood_fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for hidden_units in sorted(log_likelihood_fnn.keys()):\n",
    "    make_plot(log_likelihood_fnn[hidden_units], trX.shape[0], batch_size,100,\\\n",
    "              label='%i'%hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temperatures=np.array(sorted(list(set(test['temperature']))))\n",
    "Pred_avgs=[]\n",
    "num_T=Temperatures.shape[0]\n",
    "num_test=len(test['temperature'])\n",
    "batch=num_test/num_T\n",
    "\n",
    "for i in xrange(num_T):\n",
    "    start=batch*i\n",
    "    end=start+batch\n",
    "    batch_prediction=fnn.predict(teX[start:end], teY[start:end], prob=True)\n",
    "    Pred_avgs.append(np.mean(batch_prediction,axis=0))\n",
    "    \n",
    "Pred_avgs=np.array(Pred_avgs)\n",
    "\n",
    "# Temperatures=Temperatures[::2]\n",
    "# Pred_avgs=np.array(Pred_avgs[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Temperatures, Pred_avgs[:,0], 'bo', Temperatures, Pred_avgs[:,0], 'b-', \\\n",
    "         Temperatures, Pred_avgs[:,1], 'ro', Temperatures, Pred_avgs[:,1], 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_units=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=0.05, activation='logistic',\n",
    "...                     hidden_layer_sizes=(hidden_units,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(trX,trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=clf.predict_proba(teX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = np.argmax(y_pred,1)==np.argmax(trY,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temperatures=np.array(sorted(list(set(test['temperature']))))\n",
    "Pred_avgs=[]\n",
    "num_T=Temperatures.shape[0]\n",
    "num_test=len(test['temperature'])\n",
    "batch=num_test/num_T\n",
    "\n",
    "for i in xrange(num_T):\n",
    "    start=batch*i\n",
    "    end=start+batch\n",
    "    batch_prediction=y_pred[start:end]\n",
    "    Pred_avgs.append(np.mean(batch_prediction,axis=0))\n",
    "\n",
    "Pred_avgs=np.array(Pred_avgs)\n",
    "Temperatures=Temperatures[::2]\n",
    "Pred_avgs=np.array(Pred_avgs[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(Temperatures, Pred_avgs[:,0], 'bo', Temperatures, Pred_avgs[:,0], 'b-', \\\n",
    "         Temperatures, Pred_avgs[:,1], 'ro', Temperatures, Pred_avgs[:,1], 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(hidden_units=3, lamb=0.05,lr=0.001):\n",
    "    classifier=Sequential()\n",
    "    classifier.add(Dense(output_dim=hidden_units, init='uniform', kernel_regularizer=regularizers.l2(lamb), \\\n",
    "                     activation='sigmoid',input_dim=trX.shape[1]))\n",
    "    classifier.add(Dense(output_dim=1, init='uniform', kernel_regularizer=regularizers.l2(lamb), \\\n",
    "                     activation='sigmoid'))\n",
    "    sgd = SGD(lr=0.001, decay=1e-6)#, momentum=0.9, nesterov=True)\n",
    "    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "classifier=KerasClassifier(build_fn=create_model,batch_size=100, nb_epoch=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'hidden_units':[400],\n",
    "             'lamb':[0.0,0.001,0.01],}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = grid_search.fit(np.array(trX),np.argmax(trY,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier=create_model(hidden_units=3, lamb=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classifier=Sequential()\n",
    "# # classifier.add(Dense(output_dim=400, init='uniform', activation='sigmoid',input_dim=trX.shape[1]))\n",
    "# # classifier.add(Dense(output_dim=2, init='uniform', activation='sigmoid'))\n",
    "# # with regularization\n",
    "# lamb=0.002\n",
    "# hidden_units=400\n",
    "# classifier.add(Dense(output_dim=hidden_units, init='uniform', kernel_regularizer=regularizers.l2(lamb), \\\n",
    "#                      activation='sigmoid',input_dim=trX.shape[1]))\n",
    "# classifier.add(Dense(output_dim=2, init='uniform', kernel_regularizer=regularizers.l2(lamb), \\\n",
    "#                      activation='sigmoid'))\n",
    "\n",
    "# sgd = SGD(lr=0.001, decay=1e-6)#, momentum=0.9, nesterov=True)\n",
    "# classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# classifier.fit(np.array(trX),np.array(trY),batch_size=100, nb_epoch=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.fit(np.array(trX),np.argmax(trY,axis=1),batch_size=100, nb_epoch=10,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(np.array(teX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean((y_pred>0.5)[:,0]==np.argmax(teY,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.argmax(y_pred,axis=1)==np.argmax(teY,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Temperatures=np.array(sorted(list(set(test['temperature']))))\n",
    "Pred_avgs=[]\n",
    "num_T=Temperatures.shape[0]\n",
    "num_test=len(test['temperature'])\n",
    "batch=num_test/num_T\n",
    "\n",
    "for i in xrange(num_T):\n",
    "    start=batch*i\n",
    "    end=start+batch\n",
    "    batch_prediction=y_pred[start:end]\n",
    "    Pred_avgs.append(np.mean(batch_prediction,axis=0))\n",
    "\n",
    "Pred_avgs=np.array(Pred_avgs)\n",
    "# Temperatures=Temperatures[::2]\n",
    "# Pred_avgs=np.array(Pred_avgs[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(Temperatures, Pred_avgs[:,0], 'bo', Temperatures, Pred_avgs[:,0], 'b-', \\\n",
    "         Temperatures, Pred_avgs[:,1], 'ro', Temperatures, Pred_avgs[:,1], 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(Temperatures, Pred_avgs[:,0], 'bo', Temperatures, Pred_avgs[:,0], 'b-', \\\n",
    "         Temperatures, 1-Pred_avgs[:,0], 'ro', Temperatures, 1-Pred_avgs[:,0], 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
